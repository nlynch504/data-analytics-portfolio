param_grid = {'n_neighbors':np.arange(1,10)}
knn = KNeighborsClassifier()
knn_cv = GridSearchCV(knn, param_grid, cv=5)
knn_cv.fit(X_train,y_train)

print("Tuned KNN Parameters: {}".format(knn_cv.best_params_))
print("Best score is {}".format(knn_cv.best_score_))

Tuned KNN Parameters: {'n_neighbors': 46}
Best score is 0.8830734350807343

KNN = 8 also works well around .88 where 46 seems to overfit the data

#confusion matrix

[[7910   76]
 [ 989   68]]
             precision    recall  f1-score   support

          0       0.89      0.99      0.94      7986
          1       0.47      0.06      0.11      1057

avg / total       0.84      0.88      0.84      9043


####### random forest
param_grid = {"n_estimators": np.arange(1, 100, 20),
             "max_features":np.arange(5,25,5),
             "criterion": ["gini", "entropy"]
    }

Tuned rf Parameters: {'criterion': 'gini', 'max_features': 10, 'n_estimators': 81}
Best score is 0.8932481751824818

####### confusion matrix
[[7834  152]
 [ 817  240]]
 # classification report
             precision    recall  f1-score   support

          0       0.91      0.98      0.94      7986
          1       0.61      0.23      0.33      1057

avg / total       0.87      0.89      0.87      9043


###### nb output
Tuned nb Parameters: {'alpha': 0.05}
Best score is 0.8357111258571113
